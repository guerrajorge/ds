{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import re\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(\"samples/samples.json\", format='json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('datatype columns')\n",
    "df.printSchema()\n",
    "\n",
    "print('first 5 observations')\n",
    "df.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sparkContext object\n",
    "sc = spark.sparkContext\n",
    "# read the file\n",
    "rdd = sc.textFile('samples/samples.custom')\n",
    "# split each line and obtain 3 columns: user_id, feature_9 and feature_10\n",
    "rdd = rdd.map(lambda line: re.split('user_id=|feature_9=|feature_10='', line)[1:])\n",
    "# convert rdd to dataframe\n",
    "feat_9_10_df = rdd.map(lambda line: Row(user_id=line[0], feature_9=line[1], feature_10=line[2])).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sparkContext object\n",
    "sc = spark.sparkContext\n",
    "# read the file\n",
    "rdd = sc.textFile(\"samples/samples.tsv\")\n",
    "# split each line and obtain user_id and labels\n",
    "label_rdd = rdd.map(lambda line: line.split(\"\\t\"))\n",
    "# convert label_rdd to df\n",
    "label_df = label_rdd.map(lambda line: Row(user_id=line[0], label=line[1])).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('merging json and custom')\n",
    "# df_n_cols, df_n_rows = len(df.columns), df.count()\n",
    "# print('number of cols = {0}, rows={1}'.format(df_n_cols, df_n_rows))\n",
    "# merge df with the rest of the features df based on user_id\n",
    "df_2 = df.join(feat_9_10_df,['user_id'],'inner')\n",
    "# df_2_n_cols, df_2_n_rows = len(df_2.columns), df_2.count()\n",
    "# sanity check\n",
    "# print('number of cols = {0}, rows={1}'.format(df_2_n_cols, df_2_n_rows))\n",
    "\n",
    "# if df_2_n_rows == df_n_rows:\n",
    "#     print('same user_id found in both files')\n",
    "# else:\n",
    "#     print('diff user_id found in both files')\n",
    "\n",
    "print('df schema after merge')\n",
    "df_2.printSchema()\n",
    "print('first 5 observations')\n",
    "df_2.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('merging json + custom and tsv')\n",
    "# merge new df with the labels df based on user_id\n",
    "dataset = df_2.join(label_df,['user_id'],'inner')\n",
    "# dataset_n_cols, dataset_n_rows = len(dataset.columns), dataset.count()\n",
    "# sanity check\n",
    "# print('number of cols = {0}, rows={1}'.format(dataset_n_cols, dataset_n_rows))\n",
    "\n",
    "# if dataset_n_rows == df_n_rows:\n",
    "#     print('same user_id found in both files')\n",
    "# else:\n",
    "#     print('diff user_id found in both files')\n",
    "\n",
    "print('df schema after merge')\n",
    "dataset.printSchema()\n",
    "print('first 5 observations')\n",
    "dataset.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO:\n",
    "# 1. check for types of variables:\n",
    "    # - Useless = unique, discrete data with no potential relationship with the outcome variable. A useless feature has high cardinality.\n",
    "    # - Ratio (equal spaces between values and a meaningful zero value — mean makes sense)\n",
    "    # - Interval (equal spaces between values, but no meaningful zero value — mean makes sense)\n",
    "    # - Ordinal (first, second, third values, but not equal space between first and second and second and third — median makes sense)\n",
    "    # - Nominal (no numerical relationship between the different categories — mean and median are meaningless). You can one-hot-encode or hash nominal features. Do not ordinal encode them because the relationship between the groups cannot be reduced to a monotonic function. The assigning of values would be random."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
